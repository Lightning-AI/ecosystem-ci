jobs:

- job: check_diff
  pool:
    vmImage: 'Ubuntu-20.04'
  steps:
    - bash: |
        pip --version
        pip install -q -r requirements.txt
        pip list
      displayName: 'Install dependencies'

    - script: |
        echo $PR_NUMBER
        CONFIGS=$(python actions/assistant.py changed_configs $PR_NUMBER --as_list=False 2>&1)
        printf "Changed configs: $CONFIGS\n"
        echo "##vso[task.setvariable variable=diff;isOutput=true]$CONFIGS"
      name: files
      env:
        PR_NUMBER: "$(System.PullRequest.PullRequestNumber)"
      displayName: 'Config diff'


- ${{ each config in parameters.configs }}:
  - job:
    displayName: ${{config}}
    dependsOn: check_diff
    variables:
      # map the output variable from A into this job
      configs: $[ dependencies.check_diff.outputs['files.diff'] ]
      config: "configs/${{ config }}"
    condition: or(eq(variables['Build.SourceBranch'], 'refs/heads/main'), contains(variables['configs'], variables['config']))
    # how long to run the job before automatically cancelling
    timeoutInMinutes: 75
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: 2
    workspace:
      clean: all

    pool: gridai-spot-pool
    # this need to have installed docker in the base image...
    container:
      # base ML image: mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04
      # image: "pytorchlightning/ecosystem-ci:pt21.11"
      # image: "nvcr.io/nvidia/pytorch:21.11-py3"
      image: "pytorch/pytorch:1.8.1-cuda11.1-cudnn8-devel"
      options: "-it --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all --shm-size=32g"
    steps:

    - bash: |
        # lspci | egrep 'VGA|3D'
        # whereis nvidia
        nvidia-smi
        python --version
      displayName: 'Image info & NVIDIA'

    - bash: |
        pip --version
        pip install --requirement requirements.txt
        pip list
      displayName: 'Install dependencies'

    - bash: |
        python -c "import torch ; mgpu = torch.cuda.device_count() ; assert mgpu > 0, f'GPU: {mgpu}'"
      displayName: 'Sanity check'

    - bash: |
        python actions/assistant.py prepare_env --config_file=configs/${{config}} > prepare_env.sh
        cat prepare_env.sh
      displayName: 'Create scripts'

    - bash: |
        bash prepare_env.sh
        # ls -l
        sudo install -q -y tree
        tree .
      displayName: 'Prepare env.'

    - script: |
        ENVS=$(python actions/assistant.py list_env --config_file=configs/${{config}} 2>&1)
        printf "PyTest env. variables: $ENVS\n"
        echo "##vso[task.setvariable variable=envs;isOutput=true]$ENVS"
        ARGS=$(python actions/assistant.py specify_tests --config_file=configs/${{config}} 2>&1)
        printf "PyTest arguments: $ARGS\n"
        echo "##vso[task.setvariable variable=args;isOutput=true]$ARGS"
      name: testing
      displayName: 'testing specs'

    - bash: |
        printf "${PYTEST_ENVS}\n"
        printf "${PYTEST_ARGS}\n"
        ${PYTEST_ENVS} coverage run -m pytest ${PYTEST_ARGS} -v
      workingDirectory: _integrations
      env:
        PYTEST_ENVS: $(testing.envs)
        PYTEST_ARGS: $(testing.args)
      displayName: 'Integration tests'

    # ToDo: add Slack notification
