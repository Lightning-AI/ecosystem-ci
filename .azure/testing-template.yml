jobs:

- job: check_diff
  pool:
    vmImage: 'Ubuntu-20.04'
  steps:
    - bash: |
        pip --version
        pip install -q -r requirements.txt
        pip list
      displayName: 'Install dependencies'

    - script: |
        echo $PR_NUMBER
        CONFIGS=$(python actions/assistant.py changed_configs $PR_NUMBER --as_list=False 2>&1)
        printf "Changed configs: $CONFIGS\n"
        echo "##vso[task.setvariable variable=diff;isOutput=true]$CONFIGS"
      name: files
      env:
        PR_NUMBER: "$(System.PullRequest.PullRequestNumber)"
      displayName: 'Config diff'


- ${{ each config in parameters.configs }}:
  - job:
    displayName: ${{config}}
    dependsOn: check_diff
    variables:
      # map the output variable from A into this job
      configs: $[ dependencies.check_diff.outputs['files.diff'] ]
      config: "${{ config }}"
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0,1"; print(gpus)' )

    condition: or(eq(variables['Build.SourceBranch'], 'refs/heads/main'), contains(variables['configs'], variables['config']))
    # how long to run the job before automatically cancelling
    timeoutInMinutes: 75
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: 2
    workspace:
      clean: all

    pool: 'lit-rtx-3090'
    # this need to have installed docker in the base image...
    container:
      # base ML image: mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04
      # image: "pytorchlightning/ecosystem-ci:pt1.8.1"
      # image: "nvcr.io/nvidia/pytorch:21.11-py3"
      image: "pytorch/pytorch:1.13.0-cuda11.6-cudnn8-runtime"
      options: "--gpus=all --shm-size=8g -v /usr/bin/docker:/tmp/docker:ro"
    steps:

    - bash: |
        echo "##vso[task.setvariable variable=CONTAINER_ID]$(head -1 /proc/self/cgroup|cut -d/ -f3)"
        echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
      displayName: 'Set environment variables'

    - bash: |
        whoami && id
        lspci | egrep 'VGA|3D'
        whereis nvidia
        nvidia-smi
        echo $CUDA_VISIBLE_DEVICES
        echo $CONTAINER_ID
        python --version
        pip --version
        pip list
        python -c "import torch ; print(torch.cuda.get_arch_list())"
      displayName: 'Image info & NVIDIA'

    - bash: |
        pip --version
        pip install --requirement requirements.txt
        pip list
      displayName: 'Install dependencies'

    - bash: |
        python -c "import torch ; mgpu = torch.cuda.device_count() ; assert mgpu > 0, f'GPU: {mgpu}'"
      displayName: 'Sanity check'

    - bash: |
        python actions/assistant.py prepare_env --config_file=${{config}} > prepare_env.sh
        cat prepare_env.sh
      displayName: 'Create scripts'

    - bash: |
        bash prepare_env.sh
        tree .
        pip list
      displayName: 'Prepare env.'

    - script: |
        ENVS=$(python actions/assistant.py list_env --config_file=${{config}} --export 2>&1)
        printf "PyTest env. variables: $ENVS\n"
        echo "##vso[task.setvariable variable=envs;isOutput=true]$ENVS"
        ARGS=$(python actions/assistant.py specify_tests --config_file=${{config}} 2>&1)
        printf "PyTest arguments: $ARGS\n"
        echo "##vso[task.setvariable variable=args;isOutput=true]$ARGS"
      name: testing
      displayName: 'testing specs'

    - bash: |
        $(testing.envs)
        python -m pytest $(testing.args) -v
      workingDirectory: _integrations
      displayName: 'Integration tests'

    # ToDo: add Slack notification
